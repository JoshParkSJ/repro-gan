{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment"
      ],
      "metadata": {
        "id": "z7fxsi0cRh30"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rjujyf7BRcW-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "text = open(\"data/input.txt\", \"r\")\n",
        "\n",
        "# get unique chars in training data\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# map string to integers (idx)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) } # string to int\n",
        "itos = { i:ch for i,ch in enumerate(chars) } # int to string\n",
        "\n",
        "# char encode/decode\n",
        "encode = lambda s: [stoi[c] for c in s] # string -> list of ints\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # list of ints -> string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
        "    x = torch.stack([data[i:i+context_length] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+context_length+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64\n",
        "context_length = 256\n",
        "max_iters = 5000\n",
        "eval_interval = 250\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384 # for token embedding dimension\n",
        "n_head = 6 # n_embd/n_head = 384/6 = every head is 64 dimensional (token embedding is divided evenly among the attention heads)\n",
        "n_layer = 6\n",
        "dropout = 0.2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out # (B, T, C)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # multiple heads in parallel\n",
        "        self.proj = nn.Linear(n_embd, n_embd) # residual connection\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) # output of self-attention\n",
        "        out = self.dropout(self.proj(out)) # projection back into residual pathway\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), # inner-layer has multiplier of 4 for more neurons that does computing (just performed best empirically according to \"attention is all you need\" paper)\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), # projection layer going back into residual pathway\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x)) # data communication\n",
        "        x = x + self.ffwd(self.ln2(x)) # data computation\n",
        "        return x\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(context_length, n_embd)\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,n_embd)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "        if targets is None:\n",
        "            loss = None # inference mode\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -context_length:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = LanguageModel().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss(model)\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vck9q9iYRgFz",
        "outputId": "c603aa9a-1a54-4f4e-9a88-d2446944f27c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.788929 M parameters\n",
            "step 0: train loss 4.2849, val loss 4.2819\n",
            "step 250: train loss 2.3827, val loss 2.4084\n",
            "step 500: train loss 2.0118, val loss 2.1025\n",
            "step 750: train loss 1.7486, val loss 1.8884\n",
            "step 1000: train loss 1.5967, val loss 1.7738\n",
            "step 1250: train loss 1.5063, val loss 1.6962\n",
            "step 1500: train loss 1.4340, val loss 1.6401\n",
            "step 1750: train loss 1.3815, val loss 1.6032\n",
            "step 2000: train loss 1.3405, val loss 1.5619\n",
            "step 2250: train loss 1.3071, val loss 1.5487\n",
            "step 2500: train loss 1.2760, val loss 1.5277\n",
            "step 2750: train loss 1.2468, val loss 1.5179\n",
            "step 3000: train loss 1.2248, val loss 1.5037\n",
            "step 3250: train loss 1.2054, val loss 1.4958\n",
            "step 3500: train loss 1.1857, val loss 1.4901\n",
            "step 3750: train loss 1.1636, val loss 1.4852\n",
            "step 4000: train loss 1.1464, val loss 1.4858\n",
            "step 4250: train loss 1.1277, val loss 1.4808\n",
            "step 4500: train loss 1.1100, val loss 1.4791\n",
            "step 4750: train loss 1.0905, val loss 1.4853\n",
            "step 4999: train loss 1.0741, val loss 1.4840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate text\n",
        "starting_prompt = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(starting_prompt, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "ZdokPfnEYmO3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24fd60a0-5008-4056-b15e-087934f77ced"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BUCKINGHAM:\n",
            "God call these satisfate, forewell.\n",
            "\n",
            "GEORGE:\n",
            "My lord, God, forbid an else purse vows;\n",
            "For I impt you true in your posts,\n",
            "Being the fear by yonderable, these weep,\n",
            "Or most lightly work I heart of eyes,\n",
            "And what would not see 'em the gods on my sent?\n",
            "\n",
            "BISHOP OF ELY:\n",
            "And to-day I do but onfer Montague;\n",
            "There did princely armour tied of A moriof;\n",
            "Who bidun the prince looks on 's earth. Nay, Camillo well'd\n",
            "That mayst thwice to him made use the Eas.\n",
            "\n",
            "BETH:\n",
            "Overefore, farewell.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Accout of for pride your grace my faitht.\n",
            "\n",
            "BUSHY:\n",
            "Bless a diremned to peece for disgrace I straint.\n",
            "\n",
            "DORCASE:\n",
            "Wear men, after for I do live him, save a\n",
            "voice by minister, one is land,\n",
            "And this dangerous give sorrow was with already,\n",
            "And swell his great accession can beggard,\n",
            "Lest his pardon words. Now, very heart you:\n",
            "Prone, sir, heaven she, a widoward it his honest.\n",
            "\n",
            "CAPULET:\n",
            "Ha! stable power mournists her poper.\n",
            "\n",
            "PAGE:\n",
            "Go of me your look; for he's see so, my lord.\n",
            "\n",
            "ANGELO:\n",
            "Well; I ne'er be seen, no bear her.\n",
            "\n",
            "PAULINA:\n",
            "Contain, marvel, go remend.\n",
            "\n",
            "PAULINA:\n",
            "What at the whit man woful show them against learn?\n",
            "Now, sirrah, I'll not so tuding. Look, madam;\n",
            "In pasticious echild; they have\n",
            "To break o'er the noble less.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Most high all buy\n",
            "But reason be my chirg of thy uncles\n",
            "By their in cloack.\n",
            "Alask you, to not egan to be\n",
            "A rich-cafford by foolish, I know you lie trrow!\n",
            "\n",
            "First Gentleman:\n",
            "Romeo: old Lord Nobler: say, madam.\n",
            "\n",
            "LUCIO:\n",
            "Hou, Pompey, she dia voices.\n",
            "\n",
            "PRINCE EDWARD:\n",
            "And, how farewell usurpstives! look, what the conceful\n",
            "solemned before her this that apparate he had dearth the\n",
            "worse for me that the hearing forswer: if thou ne'er\n",
            "arise, or else thou hast made the gates. From thy name,\n",
            "Lord Stanley are not advonacted worse; and the\n",
            "eatling and builtless expression which shooks as birtte honours\n",
            "That must foot into no other too and\n",
            "to him for prayers. What's not of your privace is\n",
            "too rogun governe; but me napest, by us, sir,\n",
            "Very now, by you, wind, I \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i5sH81NZg3uk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}